{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Introduction to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/machine-learning-competitions).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nName: Dehan Ammaralda Handiana\n\nThis notebook intended for explore the data and try to use other model","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:22:57.921476Z","iopub.execute_input":"2022-02-28T03:22:57.921714Z","iopub.status.idle":"2022-02-28T03:22:57.929913Z","shell.execute_reply.started":"2022-02-28T03:22:57.921681Z","shell.execute_reply":"2022-02-28T03:22:57.92915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data inputted. Now, read it into csv","metadata":{}},{"cell_type":"code","source":"home_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\nhome_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\nprint(home_data.describe())\nprint(home_data.head())\nprint(home_data.columns)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:22:57.931348Z","iopub.execute_input":"2022-02-28T03:22:57.932294Z","iopub.status.idle":"2022-02-28T03:22:58.047729Z","shell.execute_reply.started":"2022-02-28T03:22:57.932258Z","shell.execute_reply":"2022-02-28T03:22:58.047015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = home_data.isna()\npercent = (missing.sum()/missing.count()*100).sort_values(ascending=False)\nmissing_columns = percent[percent > 0].index.tolist() # Any\nprint('Columns which have missing values: \\n{0}'.format(missing_columns))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:22:58.048782Z","iopub.execute_input":"2022-02-28T03:22:58.049274Z","iopub.status.idle":"2022-02-28T03:22:58.065946Z","shell.execute_reply.started":"2022-02-28T03:22:58.049239Z","shell.execute_reply":"2022-02-28T03:22:58.065062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicates = home_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:22:58.067179Z","iopub.execute_input":"2022-02-28T03:22:58.067775Z","iopub.status.idle":"2022-02-28T03:22:58.090263Z","shell.execute_reply.started":"2022-02-28T03:22:58.067736Z","shell.execute_reply":"2022-02-28T03:22:58.089485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"home_data.drop(missing_columns, axis=1, inplace=True)\nhome_test.drop(missing_columns, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:22:58.091588Z","iopub.execute_input":"2022-02-28T03:22:58.091874Z","iopub.status.idle":"2022-02-28T03:22:58.098068Z","shell.execute_reply.started":"2022-02-28T03:22:58.091813Z","shell.execute_reply":"2022-02-28T03:22:58.097352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = (home_data.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(\"Object Columns\")\nprint(object_cols)\nprint(len(object_cols))\n\nnumerical_cols = []\nfor col in home_data.columns:\n    if (col not in object_cols):\n        numerical_cols.append(col)\n        \nprint(\"Numerical Columns\")\nprint(numerical_cols)\nprint(len(numerical_cols))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:22:58.099476Z","iopub.execute_input":"2022-02-28T03:22:58.099941Z","iopub.status.idle":"2022-02-28T03:22:58.112198Z","shell.execute_reply.started":"2022-02-28T03:22:58.099903Z","shell.execute_reply":"2022-02-28T03:22:58.111025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(8,5)\nm = 0\nfor i, column in enumerate(numerical_cols):\n    k, l = divmod(m,5)\n    ax = axes[k,l]\n    plt.sca(ax)\n    plt.scatter(home_data[column],home_data.SalePrice)\n    plt.title(column)\n    m = m + 1\nfig.set_size_inches(24,32)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:22:58.114902Z","iopub.execute_input":"2022-02-28T03:22:58.115153Z","iopub.status.idle":"2022-02-28T03:23:04.174409Z","shell.execute_reply.started":"2022-02-28T03:22:58.115118Z","shell.execute_reply":"2022-02-28T03:23:04.173815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:23:04.175623Z","iopub.execute_input":"2022-02-28T03:23:04.176028Z","iopub.status.idle":"2022-02-28T03:23:04.185114Z","shell.execute_reply.started":"2022-02-28T03:23:04.175992Z","shell.execute_reply":"2022-02-28T03:23:04.184368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import helpful libraries\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n# Load the data, and separate the target\n\ny = home_data.SalePrice\n\n# Create X (After completing the exercise, you can return to modify this line!)\nfeatures = ['OverallQual', 'GrLivArea', '1stFlrSF', '2ndFlrSF', 'TotalBsmtSF',\n            'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF']\n\n# Select columns corresponding to features, and preview the data\nX = home_data[features]\nX.head()\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n\n\n\nrf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=1, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_X, train_y)\nrf_random.best_params_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n{'n_estimators': 1800,\n 'min_samples_split': 5,\n 'min_samples_leaf': 1,\n 'max_features': 'sqrt',\n 'max_depth': 10,\n 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# Define a random forest model\nrf_model = RandomForestRegressor(n_estimators = 1800, min_samples_split = 5, min_samples_leaf = 1, \n                                 max_features = 'sqrt', max_depth = 10, bootstrap = False, random_state = 1)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:38:54.285365Z","iopub.execute_input":"2022-02-28T03:38:54.285671Z","iopub.status.idle":"2022-02-28T03:38:59.305764Z","shell.execute_reply.started":"2022-02-28T03:38:54.285639Z","shell.execute_reply":"2022-02-28T03:38:59.30501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nrf_model_on_full_data = RandomForestRegressor(n_estimators = 1800, min_samples_split = 5, min_samples_leaf = 1, \n                                 max_features = 'sqrt', max_depth = 10, bootstrap = False)\n\n\n# fit rf_model_on_full_data on all data from the training data\nrf_model_on_full_data.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:44:08.437518Z","iopub.execute_input":"2022-02-28T03:44:08.437784Z","iopub.status.idle":"2022-02-28T03:44:14.315043Z","shell.execute_reply.started":"2022-02-28T03:44:08.437756Z","shell.execute_reply":"2022-02-28T03:44:14.314272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, read the file of \"test\" data, and apply your model to make predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny = home_data.SalePrice\nfeatures = ['OverallQual', 'GrLivArea', '1stFlrSF', '2ndFlrSF', 'TotalBsmtSF',\n            'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF']\n\n\n# create test_X which comes from test_data but includes only the columns you used for prediction.\n# The list of columns is stored in a variable called features\ntest_X = home_test[features]","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:44:18.584407Z","iopub.execute_input":"2022-02-28T03:44:18.584993Z","iopub.status.idle":"2022-02-28T03:44:18.591648Z","shell.execute_reply.started":"2022-02-28T03:44:18.584955Z","shell.execute_reply":"2022-02-28T03:44:18.590719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_X.describe()\ntest_X.isnull().sum()\ntest_X.fillna(0, inplace=True)\ntest_X.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:44:20.550364Z","iopub.execute_input":"2022-02-28T03:44:20.550931Z","iopub.status.idle":"2022-02-28T03:44:20.581068Z","shell.execute_reply.started":"2022-02-28T03:44:20.550897Z","shell.execute_reply":"2022-02-28T03:44:20.580348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions which we will submit. \ntest_preds = rf_model_on_full_data.predict(test_X)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:44:22.699934Z","iopub.execute_input":"2022-02-28T03:44:22.700408Z","iopub.status.idle":"2022-02-28T03:44:23.059316Z","shell.execute_reply.started":"2022-02-28T03:44:22.700361Z","shell.execute_reply":"2022-02-28T03:44:23.058616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate a submission\n\nRun the code cell below to generate a CSV file with your predictions that you can use to submit to the competition.","metadata":{}},{"cell_type":"code","source":"# Run the code to save predictions in the format used for competition scoring\n\noutput = pd.DataFrame({'Id': home_test.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:44:24.845201Z","iopub.execute_input":"2022-02-28T03:44:24.845473Z","iopub.status.idle":"2022-02-28T03:44:24.857684Z","shell.execute_reply.started":"2022-02-28T03:44:24.845439Z","shell.execute_reply":"2022-02-28T03:44:24.85689Z"},"trusted":true},"execution_count":null,"outputs":[]}]}